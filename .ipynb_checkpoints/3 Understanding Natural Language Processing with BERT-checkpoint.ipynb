{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97eac2b0",
   "metadata": {},
   "source": [
    "## Introduction to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57bdf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a vanilla BERT-base model. \n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa999e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f8436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "named_params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(named_params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in named_params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Encoder ====\\n')\n",
    "for p in named_params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in named_params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pooler is a separate linear and tanh activated layer that acts on the [CLS] token's representation\n",
    "# This pooled_output is often used as a representation for the entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e411b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the bert-base uncased tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('Sinan loves a beautiful day')  # tokenize a simple sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tokens through the model\n",
    "\n",
    "#1 Turn tokens_with_unknown_words into a tensor (will be size (8,))\n",
    "#2 Unsqueeze a first dimension to simulate batches. Resulting shape is (1, 8)\n",
    "response = model(torch.tensor(tokenizer.encode('Sinan loves a beautiful day')).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4299db",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b12732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding for each token, the first one being the [CLS] token\n",
    "response.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39199fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This layer is trained on top of the Embedding of the CLS token\n",
    "\n",
    "response.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c629f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the final encoder's representation of the CLS token\n",
    "CLS_embedding = response.last_hidden_state[:, 0, :].unsqueeze(0)\n",
    "\n",
    "CLS_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d061f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pooler(CLS_embedding).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80af031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the embedding for CLS through the pooler gives the same output as the \"pooler_output\"\n",
    "(model.pooler(CLS_embedding) == response.pooler_output).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da451674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45091eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = 0\n",
    "for p in model.parameters():\n",
    "    if len(p.shape) == 2:\n",
    "        total_params += p.shape[0] * p.shape[1]\n",
    "        \n",
    "print(f'Total Parameters: {total_params:,}')  # This is where the 110M parameter comes from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21daca46",
   "metadata": {},
   "source": [
    "## Wordpiece tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee92760",
   "metadata": {},
   "source": [
    "## Let's start by taking a look at the Bert Tokenizer.\n",
    "\n",
    "Let's use the `from_pretrained` method to grab the uncased bert-base tokenizer\n",
    "\n",
    "A list of all available modules can be found on their site: https://huggingface.co/transformers/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dbc4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the bert-base uncased tokenizer. Quick check what does \"uncased\" mean?\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(f'Length of BERT base vocabulary: {len(tokenizer.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A simple sentence!\"\n",
    "\n",
    "tokens = tokenizer.encode(text)  # get token ids per BERT-base's vocabulary\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode will re-construct the sentence with the added [CLS] and [SEP] token\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff94fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My friend told me about this class and I love it so far! She was right.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289885d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nicer printout  of token ids and token strings\n",
    "\n",
    "print(f'Text: {text}. Num tokens: {len(tokens)}')\n",
    "for t in tokens:\n",
    "    print(f'Token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252eaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinan is not in our vocab :'(\n",
    "\n",
    "'sinan' in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2710f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_unknown_words = 'Sinan loves a beautiful day'\n",
    "tokens_with_unknown_words = tokenizer.encode(text_with_unknown_words)\n",
    "\n",
    "# We see our sub words in action!\n",
    "for t in tokens_with_unknown_words:\n",
    "    print(f'Token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ccf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('sinan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813dfcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa51549",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_unknown_words = 'Sinan is our instructor for this awesomesauce class'\n",
    "tokens_with_unknown_words = tokenizer.encode(text_with_unknown_words)\n",
    "\n",
    "for t in tokens_with_unknown_words:\n",
    "    print(f'Token: {t}, subword: {tokenizer.decode([t])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My friend told me about this class and I love it so far! She was right.\"\n",
    "\n",
    "# encode_plus gives us token ids, attention mask and segment ids (A vs B). Useful for training time\n",
    "tokens = tokenizer.encode_plus(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(text)  # calling the tokenizer directly does the same thing as encode_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617ac86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc199b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb04564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python is the 6th token (don't forget the [CLS] token!)\n",
    "python_pet = tokenizer.encode('I love my pet python')\n",
    "\n",
    "# python is the 6th token (don't forget the [CLS] token!)\n",
    "python_language = tokenizer.encode('I love coding in python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a197bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57090b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextful embedding of 'python' in 'I love my pet python'\n",
    "python_pet_embedding = model(torch.tensor(python_pet).unsqueeze(0))[0][:,5,:].detach().numpy()\n",
    "\n",
    "# contextful embedding of 'python' in 'I love coding in python'\n",
    "python_language_embedding = model(torch.tensor(python_language).unsqueeze(0))[0][:,5,:].detach().numpy()\n",
    "\n",
    "# contextful embedding of 'snake' in 'snake'\n",
    "snake_alone_embedding = model(torch.tensor(tokenizer.encode('snake')).unsqueeze(0))[0][:,1,:].detach().numpy()\n",
    "\n",
    "# contextful embedding of 'programming' in 'programming'\n",
    "programming_alone_embedding = model(torch.tensor(tokenizer.encode('programming')).unsqueeze(0))[0][:,1,:].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc36a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_pet_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_language_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3350b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity of the representation of the word Python in a sentence about coding to the word snake\n",
    "cosine_similarity(python_language_embedding, snake_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f304c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity of the representation of the word Python in a sentence about pets to the word snake. More similar!\n",
    "cosine_similarity(python_pet_embedding, snake_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccae9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity of the representation of the word Python in a sentence about pets to the word programming\n",
    "cosine_similarity(python_pet_embedding, programming_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity of the representation of the word Python in a sentence about coding to the word programming. More similar!\n",
    "cosine_similarity(python_language_embedding, programming_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d87ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbb006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8acbcb7",
   "metadata": {},
   "source": [
    "## The many embeddings of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056d74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0cf97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679efc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "word_embeddings == context-free word embeddings\n",
    "position_embeddings == encodes word position\n",
    "token_type_embeddings == 0 or 1. Used to lookup the segment embedding\n",
    "\"\"\"\n",
    "\n",
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3143e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c144cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_phrase = 'I am Sinan'\n",
    "\n",
    "# return_tensors='pt' converts to pytorch automatically\n",
    "tokenizer.encode(example_phrase, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context-less embedding of each token in our sentence\n",
    "model.embeddings.word_embeddings(tokenizer.encode(example_phrase, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the first and last row are the same because they are the \n",
    "#  [CLS] and [SEP] reserved tokens. They are the same without context for every input\n",
    "model.embeddings.word_embeddings(tokenizer.encode('I am Matt', return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087911bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675627aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.position_embeddings  # 512 embeddings, one for each position in a max 512 input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82217746",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.LongTensor(range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748eabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.position_embeddings(torch.LongTensor(range(6)))  # positional embeddings for our example_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694dcde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.token_type_embeddings  # 2 embeddings. One for A and one for B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.LongTensor([0]*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab167901",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.token_type_embeddings(torch.LongTensor([0]*6))  # All tokens have the same embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fe9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feed forward normalization layer\n",
    "\n",
    "model.embeddings.LayerNorm(\n",
    "    model.embeddings.word_embeddings(tokenizer.encode(example_phrase, return_tensors='pt')) + \\\n",
    "    model.embeddings.position_embeddings(torch.LongTensor(range(6))) + \\\n",
    "    model.embeddings.token_type_embeddings(torch.LongTensor([0]*6))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c64cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et Voilà! The many embeddings of BERT become one embedding per token\n",
    "model.embeddings(tokenizer.encode(example_phrase, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f696a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings(tokenizer.encode(example_phrase, return_tensors='pt')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec011d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
